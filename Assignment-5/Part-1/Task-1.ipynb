{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14de1840",
   "metadata": {},
   "source": [
    "### Task 1: Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2bab33",
   "metadata": {},
   "source": [
    "#### Answer in 2–4 sentences:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef87577",
   "metadata": {},
   "source": [
    "**1. What is the core assumption of Naive Bayes?**\n",
    "\n",
    "Naive Bayes assumes that all the features in the dataset are independent of each other, given the class label. This means it treats each feature as if it doesn't influence or depend on the others, which is rarely true in real-world data — but still works surprisingly well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd25a840",
   "metadata": {},
   "source": [
    "**2. Differentiate between GaussianNB, MultinomialNB, and BernoulliNB.**\n",
    "- *GaussianNB* is used when the features are continuous and follow a normal distribution.\n",
    "\n",
    "- *MultinomialNB* is suited for discrete data like word counts in text classification.\n",
    "\n",
    "- *BernoulliNB* is best for binary/boolean features, where inputs are either 0 or 1, like word presence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e912229",
   "metadata": {},
   "source": [
    "**3. Why is Naive Bayes considered suitable for high-dimensional data?**\n",
    "\n",
    "Naive Bayes performs well in high-dimensional settings because it doesn't try to learn complex relationships between features. It’s simple, fast, and doesn't overfit easily, which makes it great for tasks like text classification where features (like words) can be in the thousands."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
