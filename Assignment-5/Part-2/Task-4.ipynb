{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dddf8c7a",
   "metadata": {},
   "source": [
    "### Task 4: Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8509c",
   "metadata": {},
   "source": [
    "#### Answer briefly:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a9074",
   "metadata": {},
   "source": [
    "**1. What is entropy and information gain?**\n",
    "\n",
    "Entropy measures the amount of uncertainty or disorder in a dataset. Information gain is the reduction in entropy after splitting the data on a feature, it helps decide which feature to split on in a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2175e",
   "metadata": {},
   "source": [
    "**2. Explain the difference between Gini Index and Entropy.**\n",
    "\n",
    "Both are used to measure impurity in decision trees. Gini is simpler and faster to compute, while entropy involves logarithms. Gini tends to isolate the most frequent class quickly, whereas entropy is more focused on the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b81c89",
   "metadata": {},
   "source": [
    "**3. How can a decision tree overfit? How can this be avoided?**\n",
    "\n",
    "A decision tree can overfit by growing too deep and memorizing noise in the training data. This can be avoided by limiting tree depth, setting minimum samples per split, or using pruning techniques."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
